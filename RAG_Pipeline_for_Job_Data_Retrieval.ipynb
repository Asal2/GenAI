{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asal2/GenAI/blob/FAISS-branch/RAG_Pipeline_for_Job_Data_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31wsGEar4Rt7"
      },
      "source": [
        "# **Installing various Packeges**\n",
        "\n",
        "The name of packages are:\n",
        "\n",
        "*   Langchain and Langgraph Packages\n",
        "*   Faiss for vector store\n",
        "*   Fastapi Uvicorn for Api calls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8EuHA-nwboH",
        "outputId": "d58c323b-f8d7-4400-a113-dcc54a58f9bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-openai langchain langgraph faiss-cpu fastapi uvicorn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSO0FpLW4rJO"
      },
      "source": [
        "# **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKPU8OBbE7kD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "import re\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import faiss\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5-7kOm24udE"
      },
      "source": [
        "# **Mounting the drive and setting OpenAi key**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLos6VDWE1xN",
        "outputId": "9c9e1415-7532-4a1b-c5ad-3ccb2b83e3f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "drive.mount('/content/drive')\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bImG4cWEE5py"
      },
      "outputs": [],
      "source": [
        "llm_provider = \"openai\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGIYqVncE_dx"
      },
      "outputs": [],
      "source": [
        "def get_llm(model=None, temperature=0):\n",
        "  if llm_provider == \"openai\":\n",
        "    return ChatOpenAI(\n",
        "        api_key=openai_api_key,\n",
        "        model=model or \"gpt-4.1-nano\",\n",
        "        temperature=temperature\n",
        "      )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWidU8fT41AV"
      },
      "source": [
        "# Testing if the LLM model works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "7SW1x_9CFB47",
        "outputId": "ae9a8cd4-c245-4a8a-ca1b-6c9f8f51c95a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'AI, or Artificial Intelligence, refers to the development of computer systems and software that can perform tasks typically requiring human intelligence. These tasks include learning from data (machine learning), understanding natural language (natural language processing), recognizing images or speech, problem-solving, and decision-making. AI aims to create machines that can simulate human cognitive functions, enabling automation and intelligent behavior across various applications such as virtual assistants, autonomous vehicles, medical diagnosis, and more.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = get_llm().invoke(\"what is AI\")\n",
        "response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjcwd21q46nD"
      },
      "source": [
        "# **Defining the file path where embeddings will be stored**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeAYnMv444YG"
      },
      "outputs": [],
      "source": [
        "StoreEmbeddingsPath = \"/content/drive/MyDrive/dataset/embeddings\"\n",
        "os.makedirs(StoreEmbeddingsPath, exist_ok=True)\n",
        "EmbedFile = os.path.join(StoreEmbeddingsPath, \"job_embeddings.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1eK0noeaKlT"
      },
      "source": [
        ">  Cleaning the text by removing html tag and making the text consistant by lowering cases\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1qqkXON7Pbh"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  if pd.isna(text):\n",
        "    return \"\"\n",
        "  text = re.sub(r\"<.*?>\", \" \", text)\n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OGmgzQO5dde"
      },
      "source": [
        "> Checking if the embedding file already exists. If the file already exits the stored data will be loaded. if there is no embedding files it will create new one. This will save cost of rerunning the entire cotent and avoids repeatly embedding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q30Mkna5auu",
        "outputId": "25604fe7-0243-417b-d8bc-295faf53e0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---No precomputed file found. Generating new chunks and embeddings---\n",
            "total chunks: 11543 chunks\n",
            "--Sucessfully generated11543 chunks and 11543 embeddings--\n",
            "Generated chunks and embeddings saved to file.\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(EmbedFile):\n",
        "  print(\"---Loading the pre-comuted chucks and embedding files---\")\n",
        "  with open(EmbedFile, \"rb\") as f:\n",
        "    saved_data = pickle.load(f)\n",
        "    all_chunks = saved_data[\"chunks\"]\n",
        "    embeddings = saved_data[\"embeddings\"]\n",
        "  print(f\"Loaded {len(all_chunks)} chunks and {len(embeddings)} embeddings.\")\n",
        "else:\n",
        "  print(\"---No precomputed file found. Generating new chunks and embeddings---\")\n",
        "\n",
        "  df = pd.read_csv(\"/content/drive/MyDrive/dataset/LF_Jobs.csv\")\n",
        "  df[\"cleaned description\"] = df.iloc[:,-1].apply(clean_text)\n",
        "\n",
        "  r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=25,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "  )\n",
        "\n",
        "  all_chunks = []\n",
        "  for _, row in df.iterrows():\n",
        "\n",
        "    job_id = row[\"ID\"]\n",
        "    title = row[\"Job Title\"]\n",
        "    company = row[\"Company Name\"]\n",
        "    location = row[\"Job Location\"]\n",
        "    level = row[\"Job Level\"]\n",
        "\n",
        "    chunks = r_splitter.split_text(row[\"cleaned description\"])\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "      all_chunks.append({\n",
        "        \"id\": f\"{job_id}_{i}\",\n",
        "        \"job_id\": job_id,\n",
        "        \"title\": title,\n",
        "        \"company\": company,\n",
        "        \"location\": location,\n",
        "        \"level\": level,\n",
        "        \"text\": chunk\n",
        "      })\n",
        "\n",
        "  print(f\"total chunks: {len(all_chunks)} chunks\")\n",
        "\n",
        "  # Inititalizing OpenAI embedding model\n",
        "\n",
        "  embedding_model = OpenAIEmbeddings(\n",
        "      api_key=openai_api_key,\n",
        "      model=\"text-embedding-3-small\"  # or \"text-embedding-3-large\"\n",
        "  )\n",
        "\n",
        "  #Generating embeddings\n",
        "\n",
        "  texts = [c[\"text\"] for c in all_chunks]\n",
        "  embeddings = embedding_model.embed_documents(texts)\n",
        "\n",
        "  #Save the generated chunks and embedding into a. file\n",
        "\n",
        "  print(f\"--Sucessfully generated {len(all_chunks)} chunks and {len(embeddings)} embeddings--\")\n",
        "  with open(EmbedFile, \"wb\") as f:\n",
        "    saved_data = {\"chunks\": all_chunks, \"embeddings\": embeddings}\n",
        "    pickle.dump(saved_data, f)\n",
        "  print(\"Generated chunks and embeddings saved to file.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYFzlaNFERBE",
        "outputId": "e9c05883-7143-4113-cd5a-2fe4e52c64f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--here is the print out--\n",
            "\n",
            "--- Inspecting the Data ---\n",
            "Total number of chunks: 11543\n",
            "Total number of embeddings: 11543\n",
            "\n",
            "First chunk's metadata:\n",
            "{'company': 'Merrill',\n",
            " 'id': 'LF0001_1',\n",
            " 'job_id': 'LF0001',\n",
            " 'level': 'Mid Level',\n",
            " 'location': 'New York, NY',\n",
            " 'text': 'physical, emotional, and financial wellness, recognizing and '\n",
            "         'rewarding performance, and how we make an impact in the communities '\n",
            "         'we serve. bank of america is committed to an in-office culture with '\n",
            "         'specific requirements for office-based attendance and which allows '\n",
            "         'for an appropriate level of flexibility for our teammates and '\n",
            "         'businesses based on role-specific considerations. at bank of '\n",
            "         'america, you can build a successful career with opportunities to '\n",
            "         'learn, grow, and make an impact. join us! job',\n",
            " 'title': 'DIR, Equities Quant'}\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "\n",
        "with open(EmbedFile, \"rb\") as f:\n",
        "    saved_data = pickle.load(f)\n",
        "print(\"--here is the print out--\")\n",
        "\n",
        "print(\"\\n--- Inspecting the Data ---\")\n",
        "print(f\"Total number of chunks: {len(saved_data['chunks'])}\")\n",
        "print(f\"Total number of embeddings: {len(saved_data['embeddings'])}\")\n",
        "print(\"\\nFirst chunk's metadata:\")\n",
        "pprint.pprint(saved_data['chunks'][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Building Vector Store. For this vector store we will use FAISS\n",
        "\n"
      ],
      "metadata": {
        "id": "hJVEtRqQJLq7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DspmdqRm-_q",
        "outputId": "a91a49b6-6ada-4b41-b996-f29f46446a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index has 11543 vectors\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix = np.array(embeddings, dtype=\"float32\")\n",
        "\n",
        "def normalize(vecs):\n",
        "  norms = np.linalg.norm(vecs, axis=1, keepdims=True)\n",
        "  return vecs / norms\n",
        "\n",
        "normalized_embeddings = normalize(embeddings) #or embedding_matrix\n",
        "\n",
        "\n",
        "\n",
        "# 3. Create FAISS index\n",
        "dimension = embedding_matrix.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension)\n",
        "index.add(normalized_embeddings)\n",
        "print(f\"FAISS index has {index.ntotal} vectors\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Building a Retriever that take a query and return the most relevant result\n",
        "\n"
      ],
      "metadata": {
        "id": "9vmdWvfYSHH6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FgP2xqNcNQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d3b64b-55d9-4950-e84b-9247a989bcf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Top 5 results for query: 'remote software engineer jobs that mention Python' ---\n",
            "\n",
            "Score: 0.5148\n",
            "Job ID: LF0192\n",
            "Title: Senior Python Data Engineer\n",
            "--- Chunk Text ---\n",
            "we seek a skilled python data engineer to join our team and help us build and maintain our data pipelines. as a python data engineer, you will be responsible for designing, developing, and testing python-based data pipelines and performing data cleansing, transformation and quality assurance. unlock the potential of remote work in uzbekistan, giving you the flexibility to work from home or access our office in tashkent. #li-dni #python-vacancies-uz #top-vacancies-10-uz #top-vacancies-10-uz-dec\n",
            "\n",
            "Score: 0.5050\n",
            "Job ID: LF0036\n",
            "Title: AI Researcher, 2025 Graduate U.S.\n",
            "--- Chunk Text ---\n",
            "country of employment at the time of hire and maintain ongoing work authorization during employment. desired skills and attributes demonstrate proficiency in python programming and ability to write clean, efficient and well-documented code. has expertise in managing data using python libraries such as numpy, pandas, matplotlib, in addition to leveraging models from hugging face and has practical knowledge of applied machine learning and deep learning frameworks, like pytorch. demonstrated\n",
            "\n",
            "Score: 0.4879\n",
            "Job ID: LF0020\n",
            "Title: Senior Data Engineer\n",
            "--- Chunk Text ---\n",
            "technical expertise, recommendations and innovative solutions work collaboratively alongside the product owner and other technologists share expertise and best practice with colleagues and contribute regularly to our engineering culture requirements ability to work in a team and collaborate with other engineers and a product owner strong core python software engineering experience good understanding and significant experience with projects using relational data models and sql strong knowledge\n",
            "\n",
            "Score: 0.4808\n",
            "Job ID: LF0240\n",
            "Title: (DoD SkillBridge) Software Engineer/Principal Software Engineer\n",
            "--- Chunk Text ---\n",
            "to the warfighter. come be a part of our mission. what you'll get to do: we are looking for a software engineer experienced in python (as your primary and most skilled in object-orientated programming language) who wants to work on an agile development team using modern development practices tackling both new development and operational support of systems that advance national security. our aurora campus has numerous programs in all phases on the development and operational lifecycle, from\n",
            "\n",
            "Score: 0.4786\n",
            "Job ID: LF0265\n",
            "Title: Senior Python Developer (with GenAI)\n",
            "--- Chunk Text ---\n",
            "prototyping coding and software development skills understanding of differences between pocs, mvps, and enterprise development approaches genuine engineering mindset, with an interest towards research &amp; development high proficiency in python is required familiarity with any of aws, azure, gcp nice to have experience with javascript we offer opportunity to work in a fast-paced, agile, software engineering culture benefit program (5 weeks of vacation, 5 paid sick days, meal vouchers,\n"
          ]
        }
      ],
      "source": [
        "def get_openai_embedding(text):\n",
        "  \"\"\"Single query embedding\"\"\"\n",
        "  return np.array(embedding_model.embed_query(text), dtype=\"float32\")\n",
        "\n",
        "\n",
        "def retrieve_relevant_chunks(query, index, chunks_list, k=5):\n",
        "  \"\"\"\n",
        "  Retrieves the top-k most relevant chunks from the FAISS index.\n",
        "\n",
        "  Args:\n",
        "    query (str): The user's search query.\n",
        "    index (faiss.Index): The FAISS index of your embeddings.\n",
        "    chunks_list (list): The original list of chunk dictionaries.\n",
        "    k (int): The number of chunks to retrieve.\n",
        "\n",
        "  Returns:\n",
        "    list: A list of dictionaries, each containing a retrieved chunk and its similarity score.\n",
        "  \"\"\"\n",
        "  # 1. Embed and normalize the user's query\n",
        "  query_vec = get_openai_embedding(query).reshape(1, -1)\n",
        "  query_vec = normalize(query_vec)\n",
        "\n",
        "  # 2. Search the FAISS index\n",
        "  # D -> distances (similarity scores), I -> indices of the vectors\n",
        "  distances, indices = index.search(query_vec, k)\n",
        "\n",
        "  # 3. Prepare and return the results\n",
        "  retrieved_chunks = []\n",
        "  for i, idx in enumerate(indices[0]):\n",
        "    if idx != -1: # FAISS returns -1 if there are not enough results\n",
        "      retrieved_chunks.append({\n",
        "          \"score\": float(distances[0][i]),\n",
        "          \"chunk\": chunks_list[idx] # Get the original chunk dictionary\n",
        "      })\n",
        "\n",
        "  return retrieved_chunks\n",
        "\n",
        "# --- Example of how to use your new retriever ---\n",
        "query = \"remote software engineer jobs that mention Python\" #Senior Python Data Engineer\n",
        "top_results = retrieve_relevant_chunks(query, index, all_chunks, k=5)\n",
        "\n",
        "print(f\"--- Top {len(top_results)} results for query: '{query}' ---\")\n",
        "for result in top_results:\n",
        "    print(f\"\\nScore: {result['score']:.4f}\")\n",
        "    print(f\"Job ID: {result['chunk']['job_id']}\")\n",
        "    print(f\"Title: {result['chunk']['title']}\")\n",
        "    print(\"--- Chunk Text ---\")\n",
        "    print(result['chunk']['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Building a RAG Chain which combines query + retrieved chunks to generate enriched responses.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EUxU_IJjSVZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7Qu1tGbcnaj",
        "outputId": "8fa473d3-5dee-41ad-9ac6-c04f60a358b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- RAG Response ---\n",
            "Yes, there are software engineer jobs that mention Python and AWS. The context indicates roles requiring solid experience in AWS services such as S3, EC2, Lambda, and IAM, as well as proficiency in writing object-oriented and/or functional programming code in Python (e.g., numpy, pandas, scipy, scikit-learn). The roles also involve containerizing and deploying code in AWS, automation and scripting tools, and knowledge of security standards. Specific details about the title, location, company, level of the job, and salary are not provided in the context.\n"
          ]
        }
      ],
      "source": [
        "llm = get_llm(model=\"gpt-4.1-nano\") # Using a faster, modern model like gpt-4o is often a good choice\n",
        "\n",
        "\n",
        "# 1. Create a function to format the retrieved documents\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n---\\n\\n\".join([d[\"chunk\"][\"text\"] for d in docs])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2. Create a prompt template\n",
        "# This instructs the LLM on how to use the retrieved context\n",
        "prompt_template = \"\"\"\n",
        "Answer the user's question based only on the following context.\n",
        "If there is the answer to user question then provide information about the question, Provide the title, location, company, level of the job and salary if available.\n",
        "If the context doesn't contain enough information, state that you can't find a relevant answer.\n",
        "Do not make up information.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Build the RAG Chain\n",
        "# This chains together the retrieval, document formatting, prompt, and LLM\n",
        "rag_chain = (\n",
        "    # This part of the chain takes the user's question\n",
        "    {\"context\": lambda x: format_docs(retrieve_relevant_chunks(x[\"question\"], index, all_chunks)), \"question\": lambda x: x[\"question\"]}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 4. Invoke the chain with a query\n",
        "query = \"Are there any software engineer jobs that mention Python and AWS?\"\n",
        "response = rag_chain.invoke({\"question\": query})\n",
        "\n",
        "print(\"--- RAG Response ---\")\n",
        "print(response)\n",
        "\n",
        "# # Example with another query\n",
        "# query_2 = \"What are the responsibilities for a data scientist at Quantum Solutions?\"\n",
        "# response_2 = rag_chain.invoke({\"question\": query_2})\n",
        "\n",
        "# print(\"\\n--- RAG Response 2 ---\")\n",
        "# print(response_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vm8V2ze4bDd"
      },
      "source": [
        "# API Endpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCn3ivJEwnDL"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnuPwVX8F4Z3"
      },
      "outputs": [],
      "source": [
        "app = FastAPI()\n",
        "\n",
        "#Request Body\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "  query: str\n",
        "  k: int = 5\n",
        "\n",
        "@app.post(\"/api/query\")\n",
        "def query_jobs(request: QueryRequest):\n",
        "  results = retrieve_faiss_openai(request.query, index, all_chunks, k=request.k)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3rtupASvm-I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcM15/kGQIam/r2MmMvMnw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}